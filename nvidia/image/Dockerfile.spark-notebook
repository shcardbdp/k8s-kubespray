ARG  DEVICE_TYPE=gpu
ARG  BASE_VERSION
FROM shcardbdp/dl-base-$DEVICE_TYPE:$BASE_VERSION

ARG   VERSION
ARG   DEVICE_TYPE
LABEL version=$VERSION device-type=$DEVICE_TYPE

USER root

# Configure environment
ENV LANG=C.UTF-8 LC_ALL=C.UTF-8
ENV CONDA_DIR /opt/conda 
ENV PATH $CONDA_DIR/bin:$PATH
ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH

USER $NB_UID

# Install conda as jovyan and check the md5 sum provided on the download site
ENV MINICONDA_VERSION 4.3.30
LABEL MINICONDA=$MINICONDA_VERSION
RUN cd /tmp && \
    wget --quiet https://repo.continuum.io/miniconda/Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh && \
    echo "0b80a152332a4ce5250f3c09589c7a81 *Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh" | md5sum -c - && \
    /bin/bash Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh -f -b -p $CONDA_DIR && \
    rm Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh && \
    $CONDA_DIR/bin/conda config --system --prepend channels conda-forge && \
    $CONDA_DIR/bin/conda config --system --set auto_update_conda false && \
    $CONDA_DIR/bin/conda config --system --set show_channel_urls true && \
    $CONDA_DIR/bin/conda update --all --quiet --yes && \
    conda clean -tipsy && \
    echo "export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH" >> /home/$NB_USER/.bashrc && \
    echo "export PATH=$JAVA_HOME/bin:$ORACLE_HOME/bin:$PATH" >> /home/$NB_USER/.bashrc && \
    rm -rf /home/$NB_USER/.cache/yarn && \
    fix-permissions $CONDA_DIR && \
    fix-permissions /home/$NB_USER

ADD connector/dbi.py /opt/conda/lib/python3.6/site-packages/shcard/    

RUN  if [ "$DEVICE_TYPE" = "gpu" ] ; then  conda install --quiet --yes tensorflow-gpu==1.12.0 keras==2.2.4 pytorch torchvision -c pytorch ; conda install --yes --quiet -c lukepfister pycuda==2017.1 scikits.cuda; else conda install --quiet --yes tensorflow==1.12.0 keras==2.2.4 pytorch; fi \
     && conda clean -tipsy  \
     && fix-permissions $CONDA_DIR && fix-permissions /home/$NB_USER

# Install Jupyter Notebook and Hub
# pytorch requires cudatoolkit and cudnn to be installed
RUN conda install --quiet --yes \
        jupyter \
        notebook \    
        jupyterlab \
        seaborn==0.9.0 \  
        matplotlib==3.0.2 \
        scikit-learn==0.20.2 \
        numpy==1.16.0 \
        pandas==0.23.4 \
        pyspark==2.3 \
        statsmodels==0.9.0 \ 
        scipy==1.1.0 \
        cython==0.29.3 \
        pytest==4.1.1 \
        numba==0.42.0 \
        bokeh==1.0.4 \
        # gensim \ Install using pip by request from Song G 
        nltk==3.4 \
        # fasttext \ Install using pip by request from Song G 
        implicit==0.3.7 \
        lightfm==1.15 \
        imbalanced-learn==0.4.3 \
        beautifulsoup4==4.7.1 \
        autopep8==1.4.3 \
        cx_oracle==7.0.0 \
        feather-format \
        swig \
        openmpi \
        pydotplus==2.0.2 \
        networkx==2.2 \
        plotly==3.5.0 \
        pycrypto \
        hyperopt==0.1.2 \
    && conda clean -tipsy && \        
    rm -rf $CONDA_DIR/share/jupyter/lab/staging && \
    rm -rf /home/$NB_USER/.cache/yarn && \
    fix-permissions $CONDA_DIR && \
    fix-permissions /home/$NB_USER

RUN pip install --quiet --no-cache-dir \
        xgboost==0.81 \
        eli5==0.8.1 \ 
        mlxtend==0.15.0.0 \
        lightgbm==2.2.2 \
        catboost==0.12.2 \
        plotnine==0.5.1 \
        missingno==0.4.1 \
        pyLDAvis==2.1.2 \
        Edward \
        dfply \
        dplython \
        #auto-sklearn \  # when it's compatible with numpy  
        #https://s3.amazonaws.com/h2o-release/datatable/stable/datatable-0.6.0/datatable-0.6.0-cp36-cp36m-linux_x86_64.whl \
    && \
    rm -rf /home/$NB_USER/.cache/yarn 

RUN pip install --quiet --no-cache-dir \
        surprise==0.1 \
        tensorboard==1.12.2 \
        tensorflowonspark==1.4.2 \
        tensorboard_logger==0.1.0 \
        tensorboard_pytorch==0.7.1 \
        tensorboardX==1.6 \
        horovod==0.15.2 \
        ipython-memory-usage==1.1 \
        #jupyter_contrib_nbextensions \ # Because of ImportError, change to install by conda
        nbresuse==0.3.0 \   
        gensim==3.7.0 \
        fasttext==0.8.3 \
        konlpy==0.5.1 \
        pycm==1.8 \
        apyori==1.1.1 \
        # toree==0.3.0 \
        #https://s3.amazonaws.com/h2o-release/datatable/stable/datatable-0.6.0/datatable-0.6.0-cp36-cp36m-linux_x86_64.whl \
    && \
    rm -rf /home/$NB_USER/.cache/yarn 

# ==================================================================
# Hive/impala, odbc connector
# ------------------------------------------------------------------
RUN conda install --yes --quiet  \
        thrift \
        thrift_sasl \
        impyla==0.14.1 \
        && \
    conda clean -tipsy && \
    rm -rf /home/$NB_USER/.cache/yarn && \
    fix-permissions $CONDA_DIR && \
    fix-permissions /home/$NB_USER

RUN pip install --quiet --no-cache-dir \
        thriftpy \
        pyodbc==4.0.25 \
        cufflinks==0.14.6 \
    && \
    rm -rf /home/$NB_USER/.cache/yarn 

RUN pip uninstall --yes pyzmq && pip  install --quiet --no-cache-dir pyzmq

# Cuz of zmq. Next time, Check the reason where pyzmq is installed  
RUN conda install --quiet --yes \
        jupyter_contrib_nbextensions==0.5.1 \
    && conda clean -tipsy && \        
    rm -rf $CONDA_DIR/share/jupyter/lab/staging && \
    rm -rf /home/$NB_USER/.cache/yarn && \
    fix-permissions $CONDA_DIR && \
    fix-permissions /home/$NB_USER

# ==================================================================
# Spark Setting
# ------------------------------------------------------------------
COPY spark/hosts /tmp/
COPY spark/conf.cloudera.yarn/ /opt/conf.cloudera.yarn/
COPY spark/hive/hive-site.xml /usr/local/spark/conf/
COPY spark/spark-defaults.conf /usr/local/spark/conf/
COPY spark/DbInterface.py /opt/conda/lib/python3.6/
COPY spark/log4j.properties /usr/local/spark/conf/
# Setting env value
ENV HADOOP_CONF_DIR /opt/conf.cloudera.yarn
ENV YARN_CONF_DIR /opt/conf.cloudera.yarn

ENV SPARK_HOME /usr/local/spark
ENV PATH $SPARK_HOME/bin:${PATH}

ENV SPARK_CONF_DIR $SPARK_HOME/conf

ENV SPARK_YARN_USER_ENV='PYSPARK_PYTHON=/shcsw/anaconda3/bin/python'
ENV PYSPARK_DRIVER_PYTHON /opt/conda/bin/python
ENV PYSPARK_PYTHON /shcsw/anaconda3/bin/python    

# Add fonts for NHN fonts
COPY fonts/ /usr/share/fonts/

USER root

RUN  if [ "$DEVICE_TYPE" = "gpu" ] ; then ln -s /usr/local/cuda-9.0/lib64/libcurand.so.9.0 /usr/local/cuda-9.0/lib64/libcurand.so.8.0 ; fi
# Set up our notebook config.
# COPY  jupyter/mllight/jupyter_notebook_config.py /etc/jupyter/
COPY  jupyter/spark/jupyter_notebook_config.py /etc/jupyter/
COPY  notebooks /home/$NB_USER/notebooks/samples/
COPY  run_jupyter.sh /usr/local/bin/
RUN rm -rf /home/$NB_USER/work && chown -R $NB_USER:$NB_UID /home/$NB_USER/notebooks && \
    fix-permissions /etc/jupyter/ && fix-permissions /home/$NB_USER/notebooks/samples

# [check 대상]
# For Jobs
EXPOSE 31600 31601 31602 31603 31604 31605 31606 31607 31608 31609 31610 31611 31612 31613 31614 31615 31616 31617 31618 31619 31620 31621 \
        31622 31623 31624 31625 31626 31627 31628 31629 31630 31631 31632 31633 31634 31635 31636 31637 31638 31639 31640 31641 31642 31643 \
        31644 31645 31646 31647 31648 31649 31650 31651 31652 31653 31654 31655 31656 31657 31658 31659 31660 31661 31662 31663 31664 31665 \
        31666 31667 31668 31669 31670 31671 31672 31673 31674 31675 31676 31677 31678 31679 31680 31681 31682 31683 31684 31685 31686 31687 \
        31688 31689 31690 31691 31692 31693 31694 31695 31696 31697 31698 31699
# For Analysis (현재구조로는 이미지 생성때마다 만들어야 함) from 31300
EXPOSE 31300 31301 31302 31303 31304 31305 31306 31307 31308 31309

CMD [ "/usr/local/bin/run_jupyter.sh", "--no-browser", "--ip=0.0.0.0", "--allow-root", "--NotebookApp.token="]

# Switch back to jovyan to avoid accidental container runs as root
USER $NB_UID

