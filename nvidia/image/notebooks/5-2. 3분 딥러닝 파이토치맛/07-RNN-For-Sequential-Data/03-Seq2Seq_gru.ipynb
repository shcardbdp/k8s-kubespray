{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ->  [104, 101, 108, 108, 111]\n",
      "hola  ->  [104, 111, 108, 97]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 256  # ascii size\n",
    "x_ = list(map(ord, \"hello\"))  # convert to list of ascii codes\n",
    "y_ = list(map(ord, \"hola\"))   # convert to list of ascii codes\n",
    "print(\"hello -> \", x_)\n",
    "print(\"hola  -> \", y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(th.LongTensor(x_))\n",
    "y = Variable(th.LongTensor(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 104,  101,  108,  108,  111])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model using GRU and conventional concatenating motion.\n",
    "'''\n",
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Seq2Seq_GRU, self).__init__()\n",
    "\n",
    "        self.n_layers = 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder = nn.GRU(hidden_size, hidden_size)\n",
    "        self.decoder = nn.GRU(hidden_size * 2, hidden_size)\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Encoder inputs and states\n",
    "        initial_state = self._init_state()\n",
    "        embedding = self.embedding(inputs).unsqueeze(1)\n",
    "        encoder_output, encoder_state = self.encoder(embedding, initial_state)\n",
    "        outputs = []\n",
    "\n",
    "        decoder_state = encoder_state\n",
    "        for i in range(targets.size()[0]): \n",
    "            decoder_input = self.embedding(targets)[i].view(1,-1, self.hidden_size)\n",
    "            decoder_input = th.cat((decoder_input, encoder_state), 2)\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            projection = self.project(decoder_output)#.unsqueeze(0))\n",
    "            outputs.append(projection)\n",
    "            \n",
    "            #_, top_i = prediction.data.topk(1)\n",
    "            \n",
    "        outputs = th.stack(outputs, 1).squeeze()\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.n_layers, batch_size, self.hidden_size).zero_()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq_GRU(vocab_size, 16)\n",
    "pred = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_.append(3)\n",
    "y_label = Variable(th.LongTensor(y_[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([ 111,  108,   97,    3])\n"
     ]
    }
   ],
   "source": [
    "print(y_label.shape)\n",
    "print(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangjunyum/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: tensor(9.8896)\n",
      "o l a \u0003 \n",
      "100 loss: tensor(1.00000e-02 *\n",
      "       5.8784)\n",
      "h o l a \n",
      "200 loss: tensor(1.00000e-02 *\n",
      "       2.5069)\n",
      "h o l a \n",
      "300 loss: tensor(1.00000e-02 *\n",
      "       1.7165)\n",
      "h o l a \n",
      "400 loss: tensor(1.00000e-02 *\n",
      "       1.3201)\n",
      "h o l a \n",
      "500 loss: tensor(1.00000e-02 *\n",
      "       1.0703)\n",
      "h o l a \n",
      "600 loss: tensor(1.00000e-03 *\n",
      "       8.9372)\n",
      "h o l a \n",
      "700 loss: tensor(1.00000e-03 *\n",
      "       7.5901)\n",
      "h o l a \n",
      "800 loss: tensor(1.00000e-03 *\n",
      "       6.5043)\n",
      "h o l a \n",
      "900 loss: tensor(1.00000e-03 *\n",
      "       5.6150)\n",
      "h o l a \n",
      "1000 loss: tensor(1.00000e-03 *\n",
      "       4.8858)\n",
      "h o l a \n",
      "1100 loss: tensor(1.00000e-03 *\n",
      "       4.2929)\n",
      "h o l a \n",
      "1200 loss: tensor(1.00000e-03 *\n",
      "       3.8155)\n",
      "h o l a \n",
      "1300 loss: tensor(1.00000e-03 *\n",
      "       3.4330)\n",
      "h o l a \n",
      "1400 loss: tensor(1.00000e-03 *\n",
      "       3.1181)\n",
      "h o l a \n",
      "1500 loss: tensor(1.00000e-03 *\n",
      "       2.8503)\n",
      "h o l a \n",
      "1600 loss: tensor(1.00000e-03 *\n",
      "       2.6270)\n",
      "h o l a \n",
      "1700 loss: tensor(1.00000e-03 *\n",
      "       2.4303)\n",
      "h o l a \n",
      "1800 loss: tensor(1.00000e-03 *\n",
      "       2.2549)\n",
      "h o l a \n",
      "1900 loss: tensor(1.00000e-03 *\n",
      "       2.0972)\n",
      "h o l a \n",
      "2000 loss: tensor(1.00000e-03 *\n",
      "       1.9543)\n",
      "h o l a \n",
      "2100 loss: tensor(1.00000e-03 *\n",
      "       1.8242)\n",
      "h o l a \n",
      "2200 loss: tensor(1.00000e-03 *\n",
      "       1.7050)\n",
      "h o l a \n",
      "2300 loss: tensor(1.00000e-03 *\n",
      "       1.5954)\n",
      "h o l a \n",
      "2400 loss: tensor(1.00000e-03 *\n",
      "       1.4944)\n",
      "h o l a \n",
      "2500 loss: tensor(1.00000e-03 *\n",
      "       1.4010)\n",
      "h o l a \n",
      "2600 loss: tensor(1.00000e-03 *\n",
      "       1.3145)\n",
      "h o l a \n",
      "2700 loss: tensor(1.00000e-03 *\n",
      "       1.2343)\n",
      "h o l a \n",
      "2800 loss: tensor(1.00000e-03 *\n",
      "       1.1599)\n",
      "h o l a \n",
      "2900 loss: tensor(1.00000e-03 *\n",
      "       1.0907)\n",
      "h o l a \n",
      "3000 loss: tensor(1.00000e-03 *\n",
      "       1.0264)\n",
      "h o l a \n",
      "3100 loss: tensor(1.00000e-04 *\n",
      "       9.6664)\n",
      "h o l a \n",
      "3200 loss: tensor(1.00000e-04 *\n",
      "       9.1094)\n",
      "h o l a \n",
      "3300 loss: tensor(1.00000e-04 *\n",
      "       8.5902)\n",
      "h o l a \n",
      "3400 loss: tensor(1.00000e-04 *\n",
      "       8.1057)\n",
      "h o l a \n",
      "3500 loss: tensor(1.00000e-04 *\n",
      "       7.6533)\n",
      "h o l a \n",
      "3600 loss: tensor(1.00000e-04 *\n",
      "       7.2303)\n",
      "h o l a \n",
      "3700 loss: tensor(1.00000e-04 *\n",
      "       6.8345)\n",
      "h o l a \n",
      "3800 loss: tensor(1.00000e-04 *\n",
      "       6.4637)\n",
      "h o l a \n",
      "3900 loss: tensor(1.00000e-04 *\n",
      "       6.1160)\n",
      "h o l a \n",
      "4000 loss: tensor(1.00000e-04 *\n",
      "       5.7898)\n",
      "h o l a \n",
      "4100 loss: tensor(1.00000e-04 *\n",
      "       5.4833)\n",
      "h o l a \n",
      "4200 loss: tensor(1.00000e-04 *\n",
      "       5.1952)\n",
      "h o l a \n",
      "4300 loss: tensor(1.00000e-04 *\n",
      "       4.9242)\n",
      "h o l a \n",
      "4400 loss: tensor(1.00000e-04 *\n",
      "       4.6689)\n",
      "h o l a \n",
      "4500 loss: tensor(1.00000e-04 *\n",
      "       4.4284)\n",
      "h o l a \n",
      "4600 loss: tensor(1.00000e-04 *\n",
      "       4.2016)\n",
      "h o l a \n",
      "4700 loss: tensor(1.00000e-04 *\n",
      "       3.9876)\n",
      "h o l a \n",
      "4800 loss: tensor(1.00000e-04 *\n",
      "       3.7855)\n",
      "h o l a \n",
      "4900 loss: tensor(1.00000e-04 *\n",
      "       3.5945)\n",
      "h o l a \n",
      "5000 loss: tensor(1.00000e-04 *\n",
      "       3.4140)\n",
      "h o l a \n",
      "5100 loss: tensor(1.00000e-04 *\n",
      "       3.2433)\n",
      "h o l a \n",
      "5200 loss: tensor(1.00000e-04 *\n",
      "       3.0817)\n",
      "h o l a \n",
      "5300 loss: tensor(1.00000e-04 *\n",
      "       2.9287)\n",
      "h o l a \n",
      "5400 loss: tensor(1.00000e-04 *\n",
      "       2.7838)\n",
      "h o l a \n",
      "5500 loss: tensor(1.00000e-04 *\n",
      "       2.6465)\n",
      "h o l a \n",
      "5600 loss: tensor(1.00000e-04 *\n",
      "       2.5164)\n",
      "h o l a \n",
      "5700 loss: tensor(1.00000e-04 *\n",
      "       2.3929)\n",
      "h o l a \n",
      "5800 loss: tensor(1.00000e-04 *\n",
      "       2.2758)\n",
      "h o l a \n",
      "5900 loss: tensor(1.00000e-04 *\n",
      "       2.1646)\n",
      "h o l a \n",
      "6000 loss: tensor(1.00000e-04 *\n",
      "       2.0590)\n",
      "h o l a \n",
      "6100 loss: tensor(1.00000e-04 *\n",
      "       1.9585)\n",
      "h o l a \n",
      "6200 loss: tensor(1.00000e-04 *\n",
      "       1.8627)\n",
      "h o l a \n",
      "6300 loss: tensor(1.00000e-04 *\n",
      "       1.7699)\n",
      "h o l a \n",
      "6400 loss: tensor(1.00000e-04 *\n",
      "       1.6750)\n",
      "h o l a \n",
      "6500 loss: tensor(1.00000e-04 *\n",
      "       1.5913)\n",
      "h o l a \n",
      "6600 loss: tensor(1.00000e-04 *\n",
      "       1.5129)\n",
      "h o l a \n",
      "6700 loss: tensor(1.00000e-04 *\n",
      "       1.4385)\n",
      "h o l a \n",
      "6800 loss: tensor(1.00000e-04 *\n",
      "       1.3679)\n",
      "h o l a \n",
      "6900 loss: tensor(1.00000e-04 *\n",
      "       1.3008)\n",
      "h o l a \n",
      "7000 loss: tensor(1.00000e-04 *\n",
      "       1.2371)\n",
      "h o l a \n",
      "7100 loss: tensor(1.00000e-04 *\n",
      "       1.1766)\n",
      "h o l a \n",
      "7200 loss: tensor(1.00000e-04 *\n",
      "       1.1191)\n",
      "h o l a \n",
      "7300 loss: tensor(1.00000e-04 *\n",
      "       1.0644)\n",
      "h o l a \n",
      "7400 loss: tensor(1.00000e-04 *\n",
      "       1.0124)\n",
      "h o l a \n",
      "7500 loss: tensor(1.00000e-05 *\n",
      "       9.6306)\n",
      "h o l a \n",
      "7600 loss: tensor(1.00000e-05 *\n",
      "       9.1610)\n",
      "h o l a \n",
      "7700 loss: tensor(1.00000e-05 *\n",
      "       8.7147)\n",
      "h o l a \n",
      "7800 loss: tensor(1.00000e-05 *\n",
      "       8.2903)\n",
      "h o l a \n",
      "7900 loss: tensor(1.00000e-05 *\n",
      "       7.8869)\n",
      "h o l a \n",
      "8000 loss: tensor(1.00000e-05 *\n",
      "       7.5033)\n",
      "h o l a \n",
      "8100 loss: tensor(1.00000e-05 *\n",
      "       7.1385)\n",
      "h o l a \n",
      "8200 loss: tensor(1.00000e-05 *\n",
      "       6.7916)\n",
      "h o l a \n",
      "8300 loss: tensor(1.00000e-05 *\n",
      "       6.4617)\n",
      "h o l a \n",
      "8400 loss: tensor(1.00000e-05 *\n",
      "       6.1480)\n",
      "h o l a \n",
      "8500 loss: tensor(1.00000e-05 *\n",
      "       5.8496)\n",
      "h o l a \n",
      "8600 loss: tensor(1.00000e-05 *\n",
      "       5.5658)\n",
      "h o l a \n",
      "8700 loss: tensor(1.00000e-05 *\n",
      "       5.2959)\n",
      "h o l a \n",
      "8800 loss: tensor(1.00000e-05 *\n",
      "       5.0391)\n",
      "h o l a \n",
      "8900 loss: tensor(1.00000e-05 *\n",
      "       4.7950)\n",
      "h o l a \n",
      "9000 loss: tensor(1.00000e-05 *\n",
      "       4.5626)\n",
      "h o l a \n",
      "9100 loss: tensor(1.00000e-05 *\n",
      "       4.3417)\n",
      "h o l a \n",
      "9200 loss: tensor(1.00000e-05 *\n",
      "       4.1314)\n",
      "h o l a \n",
      "9300 loss: tensor(1.00000e-05 *\n",
      "       3.9315)\n",
      "h o l a \n",
      "9400 loss: tensor(1.00000e-05 *\n",
      "       3.7412)\n",
      "h o l a \n",
      "9500 loss: tensor(1.00000e-05 *\n",
      "       3.5603)\n",
      "h o l a \n",
      "9600 loss: tensor(1.00000e-05 *\n",
      "       3.3881)\n",
      "h o l a \n",
      "9700 loss: tensor(1.00000e-05 *\n",
      "       3.2243)\n",
      "h o l a \n",
      "9800 loss: tensor(1.00000e-05 *\n",
      "       3.0684)\n",
      "h o l a \n",
      "9900 loss: tensor(1.00000e-05 *\n",
      "       2.9201)\n",
      "h o l a \n"
     ]
    }
   ],
   "source": [
    "log = []\n",
    "for i in range(10000):\n",
    "    prediction = model(x, y)\n",
    "    loss = criterion(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_val = loss.data[0]\n",
    "    log.append(loss_val)\n",
    "    if i % 100 == 0:\n",
    "        print(\"%d loss: %s\" % (i, loss_val))\n",
    "        _, top1 = prediction.data.topk(1, 1)\n",
    "        for c in top1.squeeze().numpy().tolist():\n",
    "            print(chr(c), end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJOZUBCIiMaciEBGJORWBiEjM/X/UJ4l+Jwd+SQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ab6c9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(log)\n",
    "plt.xlim(0,150)\n",
    "plt.ylim(0,15)\n",
    "plt.ylabel('cross entropy loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# l = nn.CrossEntropyLoss()\n",
    "# i = th.randn(3, 5, requires_grad=True)\n",
    "# t = th.empty(3, dtype=th.long).random_(5)\n",
    "# print(i.shape, t.shape)\n",
    "# o = l(i, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
