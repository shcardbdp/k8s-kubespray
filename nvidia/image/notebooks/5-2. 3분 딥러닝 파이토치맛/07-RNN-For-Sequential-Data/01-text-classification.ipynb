{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 1. 영화 리뷰 감정 분석\n",
    "**RNN 을 이용해 IMDB 데이터를 가지고 텍스트 감정분석을 해 봅시다.**\n",
    "\n",
    "데이터의 순서정보를 학습한다는 점에서 RNN은 CIFAR10 같이 정적인 고정된 형태의 데이터 보다는 동영상, 자연어, 주가 변동 데이터 등의 동적인 시계열 데이터를 이용할때 퍼포먼스가 극대화됩니다.\n",
    "이번 프로젝트를 통해 가장 기본적인 자연어 처리(Natural Language Processing)작업이라고 할 수 있는 '텍스트 감정분석'(Text Sentiment Analysis)모델을 RNN을 이용해 구현하고 학습시켜 보겠습니다.\n",
    "\n",
    "이번 책에서 처음으로 접하는 텍스트 형태의 데이터셋인 IMDB 데이터셋은 50,000건의 영화 리뷰로 이루어져 있습니다.\n",
    "각 리뷰는 다수의 영어 문장들로 이루어져 있으며, 평점이 7점 이상의 긍정적인 영화 리뷰는 2로, 평점이 4점 이하인 부정적인 영화 리뷰는 1로 레이블링 되어 있습니다. 영화 리뷰 텍스트를 RNN 에 입력시켜 영화평의 전체 내용을 압축하고, 이렇게 압축된 리뷰가 긍정적인지 부정적인지 판단해주는 간단한 분류 모델을 만드는 것이 이번 프로젝트의 목표입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드 임베딩\n",
    "\n",
    "본격적으로 모델에 대한 코드를 짜보기 전, 자연어나 텍스트 데이터를 가지고 딥러닝을 할때 언제나 사용되는 ***워드 임베딩(Word Embedding)***에 대해 간단히 배워보겠습니다.\n",
    "\n",
    "IMDB 데이터셋은 숫자로 이뤄진 텐서나 벡터 형태가 아닌 순전한 자연어로 이뤄져 있습니다. 이러한 데이터를 인공신경망에 입력시키기 위해선 여러 사전처리를 해 데이터를 벡터로 나타내 줘야 합니다. 이를 위해 가장 먼저 해야 할 일은 아래와 같이 영화 리뷰들을 단어 단위의 토큰으로 나누어 주는 것입니다. 간단한 데이터셋에선 파이썬의 split(‘ ’) 함수를 써서 토크나징을 해 줘도 큰 문제는 없지만, 더 깔끔한 토크나이징을 위해 Spacy 같은 오픈소스를 사용하는걸 추천드립니다.\n",
    "\n",
    "```python\n",
    "‘It was a good movie.’ → [‘it’, ‘was’, ‘a’, ‘good’, ‘movie’]\n",
    "```\n",
    "\n",
    "그 후 영화평 속의 모든 단어는 one hot encoding 이라는 기법을 이용해 벡터로 변환됩니다. 예를 들어 데이터셋에 총 10개의 다른 단어들이 있고 ‘movie’ 라는 단어가 10개의 단어 중 3번째 단어 일 경우 'movie'는 다음과 같이 나타내어 집니다.\n",
    "\n",
    "```python\n",
    "movie = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "그 다음으로는 one hot encoding을 거친 단어 벡터를 '사전 속 낱말 수' X '임베딩 차원값' 모양의 랜덤한 임베딩 행렬(Embedding Matrix)과 행렬곱을 해 주어야 합니다. 행렬곱의 결과는 'movie'라는 단어를 대표하는 다양한 특성값을 가진 벡터입니다.\n",
    "\n",
    "워드 임베딩은 언뜻 보기에도 코드로 정의하기엔 골치아픈 동작이지만,\n",
    "다행히도 파이토치의 nn.Embedding() 함수를 사용하면 별로 어렵지 않게 이뤄낼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "그럼 본격적으로 코드를 짜 보겠습니다.  \n",
    "가장 먼저 모델 구현과 학습에 필요한 라이브러리 들을 임포트 해 줍니다.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구현과 학습에 필요한 하이퍼파라미터 들을 정의해 줍니다.\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 40\n",
    "torch.manual_seed(42)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BasicLSTM 라고 하는 RNN을 포함하는 신경망 모델을 만들어 보겠습니다. 여타 다른 신경망 모델과 같이 파이토치의 nn.Module 을 상속받습니다.\n",
    "\n",
    "```python\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        print(\"Building Basic LSTM model...\")\n",
    "```\n",
    "\n",
    "__init()__ 함수 속 가장 먼저 정의되는 변수는 RNN의 '층'이라고 할 수 있는 n_layers 입니다. 아주 복잡한 모델이 아닌 이상, n_layers는 2이하의 값으로 정의되는게 보통입니다.  \n",
    "앞에서 잠시 언급했던 nn.Embedding() 함수는 2개의 파라미터를 입력받습니다. 이 중 첫번째는 전체 데이터셋 속 모든 단어를 사전 형태로 나타냈을 때\n",
    "이 사전속 단어의 갯수라고 할 수 있는 n_vocab이라는 숫자입니다.\n",
    "두번째 파라미터는 embed 라는 숫자입니다. 이 숫자는 쉽게 말해 임베딩된 단어 텐서가 지니는 차원값 이라고 할 수 있습니다.\n",
    "즉, 한 영화 리뷰속 모든 단어가 임베딩을 거치면 영화 리뷰는 embed 만큼 특성값을 지닌 단어 텐서들이 차례대로 나열된 배열 형태로 나타내어 집니다.\n",
    "\n",
    "```python\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "```\n",
    "\n",
    "다음으로 drop out 을 정의해 주고 본격적으로 RNN 모델을 정의합니다. 사실 원시적인 RNN은 입력받은 시계열 데이터의 길이가 길어지면 \n",
    "학습 도중 경사값(Gradient)이 너무 작아져 버리거나 너무 커져 버리는 고질적인 문제가 있었습니다.\n",
    "따라서 이러한 문제에서 조금 더 자유로운 LSTM(Long Short Term Memory)라는 RNN 을 사용하겠습니다.\n",
    "\n",
    "```python\n",
    "        self.lstm = nn.LSTM(embed_dim, self.hidden_dim,\n",
    "                            num_layers=self.n_layers,\n",
    "                            dropout=dropout_p,\n",
    "                            batch_first=True)\n",
    "```\n",
    "\n",
    "앞에서 설명했듯, RNN은 텐서의 배열을 하나의 텐서로 압축시킵니다.\n",
    "하지만 사실상 RNN의 기능은 여기서 끝나기 때문에 모델이 영화 리뷰가 긍정적인지 부정적인지 분류하는 결과값을 출력하려면 압축된 텐서를 다음과 같이 다층신경망에 입력시켜야 합니다.\n",
    "\n",
    "```python\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "본격적으로 모델에 입력된 텍스트 데이터가 어떤 전처리 과정을 거치고 신경망에 입력되는지 정의하는 forward 함수를 구현합니다.\n",
    "모델에 입력되는 데이터 x 는 한 batch 속에 있는 모든 영화평 입니다.\n",
    "이들이 embed 함수를 통해 워드 임베딩을 하게 되면 LSTM 에 입력될 수 있는 형태가 됩니다.\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  #  [b, i] -> [b, i, e]\n",
    "```\n",
    "\n",
    "보통의 신경망이라면 이제 바로 신경망 모듈의 forward 함수를 호출해도 되겠지만 \n",
    "LSTM과 같은 RNN 계열의 신경망은 입력 데이터 말고도 밑의 코드처럼 은닉 벡터(Hidden Vector)라는 텐서를 정의하고 신경망에 입력해 줘야 합니다.\n",
    "\n",
    "```python\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.lstm(x, h_0)  # [i, b, h]\n",
    "```\n",
    "\n",
    "첫번째 은닉 벡터(Hidden Vector) 인 h_0을 생성하는 _init_state 함수를 구현합니다. 꼭 그럴 필요는 없으나, 첫번째 은닉 벡터는 아래의 코드처럼 모든 특성값이 0인 벡터로 설정해 주는 것이 보통입니다.\n",
    "\n",
    "```python\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        )\n",
    "```\n",
    "\n",
    "next(self.parameters()).data 를 통해 모델 속 가중치 텐서를 weight 이라는 변수에 대입시킵니다.\n",
    "그리고 new() 함수를 이용해 weight 텐서와 같은 자료형을 갖고 있지만 (n_layers, batch_size, hidden_dim)꼴의 텐서 두개를 정의합니다. 그리고 이 두 텐서에 zero_() 함수를 호출함으로써 텐서 속 모든 원소값을 0으로 바꿔줍니다. 대부분의 RNN 계열의 신경망은 은닉 벡터를 하나만을 요구하지만, 좀 더 복잡한 구조를 가진 LSTM 은 이렇게 같은 모양의 텐서 두 개를 정의해 줘야 합니다.\n",
    "\n",
    "RNN이 만들어낸 마지막 은닉 벡터를 h_t 라고 정의하겠습니다.\n",
    "\n",
    "```python\n",
    "        h_t = x[:,-1,:]\n",
    "```\n",
    "\n",
    "이제 영화 리뷰속 모든 내용을 압축한 h_t를 다층신경망에 입력시켜 결과를 출력해야 합니다.\n",
    "\n",
    "```python\n",
    "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
    "        return logit\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구현과 신경망 학습에 필요한 함수를 구현했으면 본격적으로 IMDB 데이터셋을 가져와 보겠습니다.\n",
    "사실 아무 가공처리를 가하지 않은 텍스트 형태의 데이터셋을 신경망에 입력하는데까지는 매우 번거로운 작업을 필요로합니다.\n",
    "그러므로 우리는 이러한 전처리 작업들을 대신 해주는 Torch Text라이브러리를 사용해 IMDB 데이터셋을 가져오겠습니다.\n",
    "\n",
    "가장 먼저 텍스트 형태의 영화 리뷰들과 그에 해당하는 레이블을 텐서로 바꿔줄 때 필요한 설정사항들을 정해줘야 합니다.\n",
    "그러기 위해 이러한 설정정보를 담고있는 TEXT 와 LABEL 이라는 객체를 생성합니다. \n",
    "\n",
    "```python\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "\n",
    "```\n",
    "\n",
    "sequential 이라는 파라미터를 이용해 데이터셋이 순차적 데이터셋이라고 명시해 주고 batch_first 파라미터로 신경망에 입력되는 텐서의 첫번째 차원값이 batch_size 가 되도록 정해줍니다.\n",
    "마지막으로 lower 변수를 이용해 텍스트 데이터 속 모든 영문 알파벳이 소문자가 되도록 설정해 줍니다.\n",
    "\n",
    "그 다음으로는 datasets 객체의 splits 함수를 이용해 모델에 입력되는 데이터셋을 만들어줍니다.\n",
    "\n",
    "```python\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "```\n",
    "\n",
    "이제 만들어진 데이터셋을 이용해 전에 설명한 워드 임베딩에 필요한 워드 사전(Word Vocabulary)를 만들어줍니다.\n",
    "\n",
    "```python\n",
    "TEXT.build_vocab(train_data, min_freq=5)\n",
    "LABEL.build_vocab(train_data)\n",
    "```\n",
    "\n",
    "min_freq 은 학습데이터 속에서 최소한 5번 이상 등장한 단어들만을 사전속에 정의하겠다는 뜻입니다. 즉 학습 데이터 속에서 드물게 출현하는 단어는 'unk'(Unknown) 이라는 토큰으로 정의됩니다.\n",
    "\n",
    "그 다음으로는 train_data 와 test_data 에서 batch tensor 를 generate 할 수 있는 iterator 를 만들어 줍니다.\n",
    "\n",
    "```python\n",
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, test_data), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False)\n",
    "```\n",
    "\n",
    "마지막으로 사전 속 단어들의 숫자와 레이블의 수를 정해주는 변수를 만들어 줍니다.\n",
    "\n",
    "```python\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음은 train() 함수와 evaluate() 함수를 구현할 차례입니다.\n",
    "```python\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)  # index align\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if b % 100 == 0:\n",
    "            corrects = (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "            accuracy = 100.0 * corrects / batch.batch_size\n",
    "            sys.stdout.write(\n",
    "                '\\rBatch[%d] - loss: %.6f  acc: %.2f' %\n",
    "                (b, loss.item(), accuracy))\n",
    "```\n",
    "\n",
    "```python\n",
    "def evaluate(model, val_iter):\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "#         x, y = batch.text, batch.label\n",
    "        y.data.sub_(1)  # index align\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, size_average=False)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = avg_loss / size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, accuracy\n",
    "```\n",
    "\n",
    "본격적으로 학습을 시작하기 전, 모델 객체와 최적화 알고리즘을 정의합니다.\n",
    "\n",
    "```python\n",
    "model = BasicLSTM(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "```\n",
    "\n",
    "이제 학습에 필요한 모든 준비는 되었습니다. 마지막으로 학습을 하는 loop을 구현합니다.\n",
    "\n",
    "```python\n",
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, test_iter)\n",
    "    print(\"\\n[Epoch: %d] val_loss:%5.2f | acc:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "``` \n",
    "\n",
    "4장에서 배워 봤듯이, 우리가 원하는 최종 모델은 Training Loss가 아닌 Validation Loss가 최소화된 모델입니다. 다음과 같이 Validation Loss가 가장 작은 모델을 저장하는 로직을 구현합니다.\n",
    "\n",
    "```python    \n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/convcnn.pt')\n",
    "        best_val_loss = val_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 40\n",
    "torch.manual_seed(42)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        print(\"Building Basic LSTM model...\")\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embed_dim, self.hidden_dim,\n",
    "                            num_layers=self.n_layers,\n",
    "                            dropout=dropout_p,\n",
    "                            batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  #  [b, i] -> [b, i, e]\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.lstm(x, h_0)  # [i, b, h]\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        )\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train_data, min_freq=5)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, test_data), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False)\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2  \n",
    "\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "#         x, y = batch.text, batch.label\n",
    "        y.data.sub_(1)  # index align\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if b % 100 == 0:\n",
    "            corrects = (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "            accuracy = 100.0 * corrects / batch.batch_size\n",
    "            sys.stdout.write(\n",
    "                '\\rBatch[%d] - loss: %.6f  acc: %.2f' %\n",
    "                (b, loss.item(), accuracy))\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "#         x, y = batch.text, batch.label\n",
    "        y.data.sub_(1)  # index align\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, size_average=False)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = avg_loss / size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"[TRAIN]: %d \\t [TEST]: %d \\t [VOCAB] %d \\t [CLASSES] %d\"\n",
    "      % (len(train_iter),len(test_iter), vocab_size, n_classes))\n",
    "\n",
    "model = BasicLSTM(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(model)\n",
    "\n",
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, test_iter)\n",
    "\n",
    "    print(\"\\n[Epoch: %d] val_loss:%5.2f | acc:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/convcnn.pt')\n",
    "        best_val_loss = val_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hyper parameters\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 40\n",
    "torch.manual_seed(42)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print(\"\\nLoading data...\")\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train_data, min_freq=5)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# train_iter, test_iter = data.BucketIterator.splits(\n",
    "#         (train_data, test_data), batch_size=BATCH_SIZE,\n",
    "#         shuffle=True, repeat=False,device=-1)\n",
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, test_data), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False)\n",
    "\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "#len(LABEL.vocab) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]: 391 \t [TEST]: 391 \t [VOCAB] 46159 \t [CLASSES] 2\n"
     ]
    }
   ],
   "source": [
    "print(\"[TRAIN]: %d \\t [TEST]: %d \\t [VOCAB] %d \\t [CLASSES] %d\"\n",
    "      % (len(train_iter),len(test_iter), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        print(\"Building Basic GRU model...\")\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                            num_layers=self.n_layers,\n",
    "                            dropout=dropout_p,\n",
    "                            batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.gru(x, h_0)  # [i, b, h]\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "#         x, y = batch.text, batch.label\n",
    "        y.data.sub_(1)  # index align\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if b % 100 == 0:\n",
    "            corrects = (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "            accuracy = 100.0 * corrects / batch.batch_size\n",
    "            sys.stdout.write(\n",
    "                '\\rBatch[%d] - loss: %.6f  acc: %.2f' %\n",
    "                (b, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "#         x, y = batch.text, batch.label\n",
    "        y.data.sub_(1)  # index align\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, size_average=False)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = avg_loss / size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic GRU model...\n",
      "BasicGRU(\n",
      "  (embed): Embedding(46159, 128)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (gru): GRU(128, 256, batch_first=True, dropout=0.5)\n",
      "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangjunyum/anaconda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Batch[0] - loss: 0.703324  acc: 42.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5e16a91a663c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-64b463c699fd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sangjunyum/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sangjunyum/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, test_iter)\n",
    "\n",
    "    print(\"\\n[Epoch: %d] val_loss:%5.2f | acc:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicRNN(nn.Module):\n",
    "#     \"\"\"\n",
    "#         Basic RNN\n",
    "#     \"\"\"\n",
    "#     def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "#         super(BasicRNN, self).__init__()\n",
    "#         print(\"Building Basic RNN model...\")\n",
    "#         self.n_layers = n_layers\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         self.rnn = nn.RNN(embed_dim, hidden_dim, n_layers,\n",
    "#                           dropout=dropout_p, batch_first=True)\n",
    "#         self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embed(x)  #  [b, i] -> [b, i, e]\n",
    "#         _, hidden = self.rnn(embedded)\n",
    "#         self.dropout(hidden)\n",
    "#         hidden = hidden.squeeze()\n",
    "#         logit = self.out(hidden)  # [b, h] -> [b, o]\n",
    "#         return logit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
