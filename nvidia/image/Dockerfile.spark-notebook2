ARG  DEVICE_TYPE=gpu
ARG  BASE_VERSION
FROM shcardbdp/datalab-base-$DEVICE_TYPE:$BASE_VERSION

ARG   VERSION
ARG   DEVICE_TYPE
LABEL version=$VERSION device-type=$DEVICE_TYPE

USER $NB_UID

RUN echo "DEVICE_TYPE : $DEVICE_TYPE"
RUN  if [ "$DEVICE_TYPE" = "gpu" ] ; then  conda install --quiet --yes tensorflow-gpu==1.12.0 keras==2.2.4 pytorch torchvision -c pytorch ; conda install --yes --quiet -c lukepfister pycuda==2017.1 scikits.cuda; else conda install --quiet --yes tensorflow==1.12.0 keras==2.2.4 pytorch; fi \
     && conda clean -afy \
     && fix-permissions $CONDA_DIR && fix-permissions /home/$NB_USER

USER root

# Spark dependencies
ENV APACHE_SPARK_VERSION=2.4.4 \
    HADOOP_VERSION=2.7

RUN apt-get -y update && \
    apt-get install --no-install-recommends -y openjdk-8-jre-headless ca-certificates-java && \
    rm -rf /var/lib/apt/lists/*

RUN cd /tmp && \
    wget -q https://archive.apache.org/dist/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    echo "2E3A5C853B9F28C7D4525C0ADCB0D971B73AD47D5CCE138C85335B9F53A6519540D3923CB0B5CEE41E386E49AE8A409A51AB7194BA11A254E037A848D0C4A9E5 *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" | sha512sum -c - && \
    tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local --owner root --group root --no-same-owner && \
    rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
RUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark

# Spark and Mesos config
ENV SPARK_HOME=/usr/local/spark \
    PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:/home/$NB_USER/notebooks/src/shcPython/common:/shc_pkg/wisenut \
    # PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:/home/$NB_USER/notebooks/src/shcPython-prd:/home/$NB_USER/notebooks/src/shcPython/wisenut \
    SPARK_OPTS=--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info

# ==================================================================
# mecab analyzer from bitbucket
# ------------------------------------------------------------------
RUN wget --quiet http://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz -O mecab.tar.gz && \
    tar -zxf mecab.tar.gz && \
    cd mecab-0.996-ko-0.9.2 && \
    ./configure && \
    make && \ 
    make check && \
    make install && \
    ldconfig && \
    rm -rf ../mecab.tar.gz ../mecab-0.996-ko-0.9.2 

# ==================================================================
# install automake 1.11 for mecab dictionary
# ------------------------------------------------------------------
RUN sed -i 's/archive.ubuntu.com/kr.archive.ubuntu.com/g' /etc/apt/sources.list && \
    apt-get update && \
    apt-get install -y automake1.11 && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*    

# ==================================================================
# mecab dictionary from bitbucket 
# ------------------------------------------------------------------
RUN wget --quiet http://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz -O mecab-dic.tar.gz && \
    tar -zxf mecab-dic.tar.gz && \
    chown -R root:root mecab-ko-dic-2.1.1-20180720 && \ 
    cd mecab-ko-dic-2.1.1-20180720 && \
    ./autogen.sh  && \
    ./configure && \
    make && \
    make check && \
    make install && \
    ldconfig && \
    rm -rf ../mecab-dic.tar.gz && \
    mv ~/mecab-ko-dic-2.1.1-20180720/ ~/.mecab-ko-dic-2.1.1-20180720/

USER $NB_UID

# Install python lib by conda
RUN conda install --quiet --yes \
        pyspark==2.4.4 \
        pyarrow \
        shap==0.33.0 \
        scikit-plot==0.3.7 \
        pandas-profiling==2.3.0 \
    && conda clean -afy && \       
    rm -rf $CONDA_DIR/share/jupyter/lab/staging && \
    rm -rf /home/$NB_USER/.cache/yarn && \
    fix-permissions $CONDA_DIR && \
    fix-permissions /home/$NB_USER \
    && find /opt/conda/ -follow -type f -name '*.a' -delete \
    && find /opt/conda/ -follow -type f -name '*.pyc' -delete \
    && find /opt/conda/ -follow -type f -name '*.js.map' -delete \
    && find /opt/conda/lib/python*/site-packages/bokeh/server/static -follow -type f -name '*.js' -delete       

RUN pip install --quiet --no-cache-dir \
        tensorflowonspark==1.4.3 \        
        # tensorflowonspark \        
        tensorboard_logger==0.1.0 \       
        # tensorboard_logger \       
        tensorboard_pytorch==0.7.1 \      
        # tensorboard_pytorch \      
        tensorboardX==1.8 \               
        # tensorboardX \               
        horovod==0.16.4 \       
        # 0.16.4, 0.18.0, 0.18.1
        # horovod \       
        mecab-python3==0.7 \        
        tensorflow-hub==0.5.0 \
        # keras-tuner \ # Cuz it's need to upgrade tensorflow 2.0.0, it's out.
        tensorwatch==0.8.5 \
        torchtext==0.4.0 \
        torchaudio==0.3.0 \
        pyclustering==0.9.2 \
        efficient_apriori==1.1.0 \
        cookiecutter==1.7.0 \
        shap==0.34.0 \   
        kmodes==0.10.2 \     
    && \
    rm -rf /home/$NB_USER/.cache/yarn

# Apache Toree kernel
RUN pip install --no-cache-dir \
    https://dist.apache.org/repos/dist/release/incubator/toree/0.3.0-incubating/toree-pip/toree-0.3.0.tar.gz \
    && \
    jupyter toree install --sys-prefix && \
    rm -rf /home/$NB_USER/.local && \
    fix-permissions $CONDA_DIR && \
    fix-permissions /home/$NB_USER

# ==================================================================
# Spark Setting
# ------------------------------------------------------------------
COPY spark/conf.cloudera.yarn/ /opt/conf.cloudera.yarn/
COPY spark/hive/hive-site.xml /usr/local/spark/conf/

# volume attach 방식으로 변경 (2019-05-13)
# COPY spark/spark-defaults.conf /usr/local/spark/conf/ 

COPY spark/DbInterface.py /opt/conda/lib/python3.6/
COPY spark/log4j.properties /usr/local/spark/conf/
# Setting env value
ENV HADOOP_CONF_DIR=/opt/conf.cloudera.yarn \
    YARN_CONF_DIR=/opt/conf.cloudera.yarn \
    SPARK_HOME=/usr/local/spark
ENV PATH=$SPARK_HOME/bin:${PATH} \
    SPARK_CONF_DIR=$SPARK_HOME/conf \
    SPARK_YARN_USER_ENV='PYSPARK_PYTHON=/shcsw/anaconda3/bin/python' \
    PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python \
    PYSPARK_PYTHON=/shcsw/anaconda3/bin/python \   
# PATH Setting
    # PATH=/home/$NB_USER/notebooks/data/share/gitlab-runner/shcPython-prd/jdbc:${PATH}
    PATH=/home/$NB_USER/notebooks/src/shcPython/jdbc:${PATH}

RUN echo "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH" >> /home/$NB_USER/.bashrc && \
    echo "export PATH=$PATH" >> /home/$NB_USER/.bashrc && \
    echo "export TMOUT=3600" >> /home/$NB_USER/.bashrc

USER root

COPY entrypoint_spark.sh /usr/local/bin/
ENTRYPOINT [ "/usr/bin/tini", "--", "entrypoint_spark.sh"]
CMD [ "/usr/local/bin/run_jupyter.sh", "--no-browser", "--ip=0.0.0.0", "--allow-root", "--NotebookApp.token="]

# Switch back to jovyan to avoid accidental container runs as root
USER $NB_UID

